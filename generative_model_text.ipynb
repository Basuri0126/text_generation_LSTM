{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L1Ba_bJoxvv",
        "outputId": "714d9f4d-864f-44c6-b4f0-cf4da3f22973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2_YToY-oZg9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load ascii text and covert to lowercase\n",
        "filename = \"data_extract.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()"
      ],
      "metadata": {
        "id": "6p-EY0kdo6fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "P5WmKR1bpBbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIfZPUGqpLpG",
        "outputId": "ffbfafd4-434c-4260-a560-f92ab7074c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  482999\n",
            "Total Vocab:  59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "  seq_in = raw_text[i:i + seq_length]\n",
        "  seq_out = raw_text[i + seq_length]\n",
        "  dataX.append([char_to_int[char] for char in seq_in])\n",
        "  dataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZJ9jLiipTm3",
        "outputId": "7c302065-07a0-40b4-f1ed-b636aee2e936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Patterns:  482899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)"
      ],
      "metadata": {
        "id": "QV8euW5Hps66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "m1Mc64Aapz4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "metadata": {
        "id": "_RKYF6Utp4iM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KLWlCtEp-Pc",
        "outputId": "0be6bceb-f9fb-4e64-9223-2bf8f5104951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "3771/3773 [============================>.] - ETA: 0s - loss: 2.8804\n",
            "Epoch 1: loss improved from inf to 2.88035, saving model to weights-improvement-01-2.8803.hdf5\n",
            "3773/3773 [==============================] - 58s 13ms/step - loss: 2.8803\n",
            "Epoch 2/20\n",
            "   9/3773 [..............................] - ETA: 49s - loss: 2.7781"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3771/3773 [============================>.] - ETA: 0s - loss: 2.7398\n",
            "Epoch 2: loss improved from 2.88035 to 2.73981, saving model to weights-improvement-02-2.7398.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.7398\n",
            "Epoch 3/20\n",
            "3771/3773 [============================>.] - ETA: 0s - loss: 2.6571\n",
            "Epoch 3: loss improved from 2.73981 to 2.65709, saving model to weights-improvement-03-2.6571.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.6571\n",
            "Epoch 4/20\n",
            "3771/3773 [============================>.] - ETA: 0s - loss: 2.5661\n",
            "Epoch 4: loss improved from 2.65709 to 2.56603, saving model to weights-improvement-04-2.5660.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.5660\n",
            "Epoch 5/20\n",
            "3771/3773 [============================>.] - ETA: 0s - loss: 2.4821\n",
            "Epoch 5: loss improved from 2.56603 to 2.48206, saving model to weights-improvement-05-2.4821.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.4821\n",
            "Epoch 6/20\n",
            "3770/3773 [============================>.] - ETA: 0s - loss: 2.4141\n",
            "Epoch 6: loss improved from 2.48206 to 2.41418, saving model to weights-improvement-06-2.4142.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.4142\n",
            "Epoch 7/20\n",
            "3773/3773 [==============================] - ETA: 0s - loss: 2.3612\n",
            "Epoch 7: loss improved from 2.41418 to 2.36124, saving model to weights-improvement-07-2.3612.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.3612\n",
            "Epoch 8/20\n",
            "3771/3773 [============================>.] - ETA: 0s - loss: 2.3189\n",
            "Epoch 8: loss improved from 2.36124 to 2.31886, saving model to weights-improvement-08-2.3189.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.3189\n",
            "Epoch 9/20\n",
            "3772/3773 [============================>.] - ETA: 0s - loss: 2.2828\n",
            "Epoch 9: loss improved from 2.31886 to 2.28276, saving model to weights-improvement-09-2.2828.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.2828\n",
            "Epoch 10/20\n",
            "3773/3773 [==============================] - ETA: 0s - loss: 2.2503\n",
            "Epoch 10: loss improved from 2.28276 to 2.25029, saving model to weights-improvement-10-2.2503.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.2503\n",
            "Epoch 11/20\n",
            "3772/3773 [============================>.] - ETA: 0s - loss: 2.2218\n",
            "Epoch 11: loss improved from 2.25029 to 2.22186, saving model to weights-improvement-11-2.2219.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.2219\n",
            "Epoch 12/20\n",
            "3771/3773 [============================>.] - ETA: 0s - loss: 2.1970\n",
            "Epoch 12: loss improved from 2.22186 to 2.19708, saving model to weights-improvement-12-2.1971.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.1971\n",
            "Epoch 13/20\n",
            "3770/3773 [============================>.] - ETA: 0s - loss: 2.1735\n",
            "Epoch 13: loss improved from 2.19708 to 2.17354, saving model to weights-improvement-13-2.1735.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.1735\n",
            "Epoch 14/20\n",
            "3773/3773 [==============================] - ETA: 0s - loss: 2.1541\n",
            "Epoch 14: loss improved from 2.17354 to 2.15409, saving model to weights-improvement-14-2.1541.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.1541\n",
            "Epoch 15/20\n",
            "3772/3773 [============================>.] - ETA: 0s - loss: 2.1342\n",
            "Epoch 15: loss improved from 2.15409 to 2.13421, saving model to weights-improvement-15-2.1342.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.1342\n",
            "Epoch 16/20\n",
            "3773/3773 [==============================] - ETA: 0s - loss: 2.1170\n",
            "Epoch 16: loss improved from 2.13421 to 2.11698, saving model to weights-improvement-16-2.1170.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.1170\n",
            "Epoch 17/20\n",
            "3770/3773 [============================>.] - ETA: 0s - loss: 2.1011\n",
            "Epoch 17: loss improved from 2.11698 to 2.10108, saving model to weights-improvement-17-2.1011.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.1011\n",
            "Epoch 18/20\n",
            "3771/3773 [============================>.] - ETA: 0s - loss: 2.0852\n",
            "Epoch 18: loss improved from 2.10108 to 2.08530, saving model to weights-improvement-18-2.0853.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.0853\n",
            "Epoch 19/20\n",
            "3771/3773 [============================>.] - ETA: 0s - loss: 2.0715\n",
            "Epoch 19: loss improved from 2.08530 to 2.07149, saving model to weights-improvement-19-2.0715.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.0715\n",
            "Epoch 20/20\n",
            "3772/3773 [============================>.] - ETA: 0s - loss: 2.0581\n",
            "Epoch 20: loss improved from 2.07149 to 2.05808, saving model to weights-improvement-20-2.0581.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.0581\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f0336f9bf40>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "# load ascii text and covert to lowercase\n",
        "filename = \"data_extract.txt\"\n",
        "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
        "raw_text = raw_text.lower()\n",
        "# create mapping of unique chars to integers\n",
        "chars = sorted(list(set(raw_text)))\n",
        "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
        "# summarize the loaded data\n",
        "n_chars = len(raw_text)\n",
        "n_vocab = len(chars)\n",
        "print(\"Total Characters: \", n_chars)\n",
        "print(\"Total Vocab: \", n_vocab)\n",
        "# prepare the dataset of input to output pairs encoded as integers\n",
        "seq_length = 100\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, n_chars - seq_length, 1):\n",
        "\tseq_in = raw_text[i:i + seq_length]\n",
        "\tseq_out = raw_text[i + seq_length]\n",
        "\tdataX.append([char_to_int[char] for char in seq_in])\n",
        "\tdataY.append(char_to_int[seq_out])\n",
        "n_patterns = len(dataX)\n",
        "print(\"Total Patterns: \", n_patterns)\n",
        "# reshape X to be [samples, time steps, features]\n",
        "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
        "# normalize\n",
        "X = X / float(n_vocab)\n",
        "# one hot encode the output variable\n",
        "y = to_categorical(dataY)\n",
        "# define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(y.shape[1], activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "# define the checkpoint\n",
        "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]\n",
        "# fit the model\n",
        "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7bst0c8qBHF",
        "outputId": "50e87eb7-b5fd-4a60-8d84-d10e030e0552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Characters:  482999\n",
            "Total Vocab:  59\n",
            "Total Patterns:  482899\n",
            "Epoch 1/20\n",
            "3773/3773 [==============================] - ETA: 0s - loss: 2.8731\n",
            "Epoch 1: loss improved from inf to 2.87315, saving model to weights-improvement-01-2.8731.hdf5\n",
            "3773/3773 [==============================] - 55s 14ms/step - loss: 2.8731\n",
            "Epoch 2/20\n",
            "3770/3773 [============================>.] - ETA: 0s - loss: 2.7271\n",
            "Epoch 2: loss improved from 2.87315 to 2.72707, saving model to weights-improvement-02-2.7271.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.7271\n",
            "Epoch 3/20\n",
            "3772/3773 [============================>.] - ETA: 0s - loss: 2.6397\n",
            "Epoch 3: loss improved from 2.72707 to 2.63969, saving model to weights-improvement-03-2.6397.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.6397\n",
            "Epoch 4/20\n",
            "3773/3773 [==============================] - ETA: 0s - loss: 2.5466\n",
            "Epoch 4: loss improved from 2.63969 to 2.54658, saving model to weights-improvement-04-2.5466.hdf5\n",
            "3773/3773 [==============================] - 54s 14ms/step - loss: 2.5466\n",
            "Epoch 5/20\n",
            "3772/3773 [============================>.] - ETA: 0s - loss: 2.4659\n",
            "Epoch 5: loss improved from 2.54658 to 2.46594, saving model to weights-improvement-05-2.4659.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.4659\n",
            "Epoch 6/20\n",
            "3773/3773 [==============================] - ETA: 0s - loss: 2.4033\n",
            "Epoch 6: loss improved from 2.46594 to 2.40331, saving model to weights-improvement-06-2.4033.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.4033\n",
            "Epoch 7/20\n",
            "3773/3773 [==============================] - ETA: 0s - loss: 2.3529\n",
            "Epoch 7: loss improved from 2.40331 to 2.35292, saving model to weights-improvement-07-2.3529.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.3529\n",
            "Epoch 8/20\n",
            "3771/3773 [============================>.] - ETA: 0s - loss: 2.3129\n",
            "Epoch 8: loss improved from 2.35292 to 2.31284, saving model to weights-improvement-08-2.3128.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.3128\n",
            "Epoch 9/20\n",
            "3772/3773 [============================>.] - ETA: 0s - loss: 2.2779\n",
            "Epoch 9: loss improved from 2.31284 to 2.27790, saving model to weights-improvement-09-2.2779.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.2779\n",
            "Epoch 10/20\n",
            "3770/3773 [============================>.] - ETA: 0s - loss: 2.2468\n",
            "Epoch 10: loss improved from 2.27790 to 2.24675, saving model to weights-improvement-10-2.2468.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.2468\n",
            "Epoch 11/20\n",
            "3773/3773 [==============================] - ETA: 0s - loss: 2.2193\n",
            "Epoch 11: loss improved from 2.24675 to 2.21934, saving model to weights-improvement-11-2.2193.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.2193\n",
            "Epoch 12/20\n",
            "3770/3773 [============================>.] - ETA: 0s - loss: 2.1961\n",
            "Epoch 12: loss improved from 2.21934 to 2.19612, saving model to weights-improvement-12-2.1961.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.1961\n",
            "Epoch 13/20\n",
            "3772/3773 [============================>.] - ETA: 0s - loss: 2.1729\n",
            "Epoch 13: loss improved from 2.19612 to 2.17289, saving model to weights-improvement-13-2.1729.hdf5\n",
            "3773/3773 [==============================] - 51s 13ms/step - loss: 2.1729\n",
            "Epoch 14/20\n",
            "3772/3773 [============================>.] - ETA: 0s - loss: 2.1519\n",
            "Epoch 14: loss improved from 2.17289 to 2.15193, saving model to weights-improvement-14-2.1519.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.1519\n",
            "Epoch 15/20\n",
            "3773/3773 [==============================] - ETA: 0s - loss: 2.1336\n",
            "Epoch 15: loss improved from 2.15193 to 2.13358, saving model to weights-improvement-15-2.1336.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.1336\n",
            "Epoch 16/20\n",
            "3771/3773 [============================>.] - ETA: 0s - loss: 2.1159\n",
            "Epoch 16: loss improved from 2.13358 to 2.11593, saving model to weights-improvement-16-2.1159.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.1159\n",
            "Epoch 17/20\n",
            "3773/3773 [==============================] - ETA: 0s - loss: 2.0994\n",
            "Epoch 17: loss improved from 2.11593 to 2.09942, saving model to weights-improvement-17-2.0994.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.0994\n",
            "Epoch 18/20\n",
            "3772/3773 [============================>.] - ETA: 0s - loss: 2.0839\n",
            "Epoch 18: loss improved from 2.09942 to 2.08398, saving model to weights-improvement-18-2.0840.hdf5\n",
            "3773/3773 [==============================] - 52s 14ms/step - loss: 2.0840\n",
            "Epoch 19/20\n",
            "3771/3773 [============================>.] - ETA: 0s - loss: 2.0724\n",
            "Epoch 19: loss improved from 2.08398 to 2.07242, saving model to weights-improvement-19-2.0724.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.0724\n",
            "Epoch 20/20\n",
            "3770/3773 [============================>.] - ETA: 0s - loss: 2.0575\n",
            "Epoch 20: loss improved from 2.07242 to 2.05753, saving model to weights-improvement-20-2.0575.hdf5\n",
            "3773/3773 [==============================] - 51s 14ms/step - loss: 2.0575\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f0336f6f490>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Text with an LSTM Network"
      ],
      "metadata": {
        "id": "dKqhze1duGYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"weights-improvement-20-2.0581.hdf5\"\n",
        "model.load_weights(filename)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "MZwOL1aIuZyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_to_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "metadata": {
        "id": "FjrtzVEqu2-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick a random seed\n",
        "start = np.random.randint(0, len(dataX)-1)\n",
        "pattern = dataX[start]\n",
        "print(\"Seed:\")\n",
        "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
        "# generate characters\n",
        "for i in range(1000):\n",
        "\tx = np.reshape(pattern, (1, len(pattern), 1))\n",
        "\tx = x / float(n_vocab)\n",
        "\tprediction = model.predict(x, verbose=0)\n",
        "\tindex = np.argmax(prediction)\n",
        "\tresult = int_to_char[index]\n",
        "\tseq_in = [int_to_char[value] for value in pattern]\n",
        "\tsys.stdout.write(result)\n",
        "\tpattern.append(index)\n",
        "\tpattern = pattern[1:len(pattern)]\n",
        "print(\"\\nDone.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkJNggQOu48a",
        "outputId": "b4b48d06-8b03-4865-99f9-278f1f3f2fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed:\n",
            "\" ild!” observed hester, aside to the\n",
            "minister. “o, i have much to tell thee about her! but, in\n",
            "very t \"\n",
            "ooet, and the mani of the soaee of the maniet-place, and the mani of the pani of the pane of the pane oo her breosi was the sore of the searet of the pare ofniler, whth a sore of the searet of the saarlet letter, and the mani of the pane of the saarlet letter was the sore of the searet of the pare ofniler, whth a sore of the searet of the saarlet letter, and the mani of the pane of the saarlet letter was the sore of the searet of the pare ofniler, whth a sore of the searet of the saarlet letter, and the mani of the pane of the saarlet letter was the sore of the searet of the pare ofniler, whth a sore of the searet of the saarlet letter, and the mani of the pane of the saarlet letter was the sore of the searet of the pare ofniler, whth a sore of the searet of the saarlet letter, and the mani of the pane of the saarlet letter was the sore of the searet of the pare ofniler, whth a sore of the searet of the saarlet letter, and the mani of the pane of the saarlet letter was the sore of the \n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "klwztLfLMVyR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}